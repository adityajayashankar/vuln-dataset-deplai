"""
finetuning.py
-------------
FIXES in this version:
  1. LoRA target_modules now includes MLP layers (up_proj, down_proj, gate_proj)
     Previously only attention projections were trained â€” MLP layers store factual
     knowledge, which is exactly what a CVE/security model needs to learn.
     Trainable params: ~21M â†’ ~85M. Still fits T4 (16GB) with QLoRA.

  2. Stratified train/eval split by layer field.
     Random split left thin layers (execution_context ~850 ex) with only ~42
     eval samples â€” statistically meaningless. Each layer now contributes
     proportionally to eval, so loss curves actually tell you something.

  3. Empty ### Input: block suppression.
     ~40% of synthetic pairs have input="". The old formatter still emitted
     "### Input:\n\n" â€” 16 wasted tokens per example AND taught the model
     that Input can be blank (breaks inference-time context reading).
     Now: if input is empty, the block is omitted entirely.

  4. max_length=2048 preserved from previous fix (was 1024).

Memory budget at 2048 tokens, 4-bit QLoRA, T4 16GB:
  Model (4-bit):               ~5 GB
  Activations (grad_ckpt, b=1): ~4 GB
  Optimizer (paged_adamw_8bit): ~3 GB
  Extra MLP LoRA adapters:     ~0.5 GB
  Total:                      ~12.5 GB  â†’  fits with ~3.5 GB headroom
"""

import os
import json
import torch
from pathlib import Path
from collections import defaultdict

from datasets import Dataset, DatasetDict
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
)
from peft import LoraConfig, get_peft_model, TaskType
from trl import SFTTrainer, SFTConfig

# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_MODEL   = "mistralai/Mistral-7B-Instruct-v0.3"
DATASET_PATH = Path("data") / "all_training_pairs.jsonl"
OUTPUT_DIR   = Path("./checkpoints/vuln-mistral-7b")
HF_REPO_NAME = "adityajayashankar/vuln-mistral-7b"

EVAL_FRACTION   = 0.05   # 5% of each layer goes to eval
MIN_EVAL_EXAMPLES = 30   # minimum eval examples per layer (overrides fraction for tiny layers)


# â”€â”€ FIX 3: Prompt formatter â€” suppress empty ### Input: block â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def format_example(example: dict) -> dict:
    """
    Build the training text for a single pair.
    If 'input' is empty/whitespace, omit the ### Input: section entirely.
    This saves tokens and avoids teaching the model that context can be blank.
    """
    instruction = example.get("instruction", "").strip()
    inp         = example.get("input", "").strip()
    output      = example.get("output", "").strip()

    if inp:
        text = (
            f"### Instruction:\n{instruction}\n\n"
            f"### Input:\n{inp}\n\n"
            f"### Response:\n{output}"
        )
    else:
        text = (
            f"### Instruction:\n{instruction}\n\n"
            f"### Response:\n{output}"
        )

    return {"text": text}


# â”€â”€ FIX 2: Stratified train/eval split by layer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def stratified_split(
    pairs: list[dict],
    eval_fraction: float = EVAL_FRACTION,
    min_eval: int = MIN_EVAL_EXAMPLES,
    seed: int = 42,
) -> tuple[list[dict], list[dict]]:
    """
    Split pairs into train/eval sets, sampling proportionally from each layer.

    For each layer:
      eval_n = max(min_eval, int(len(layer_pairs) * eval_fraction))
      but never more than 20% of a layer regardless of min_eval.
    """
    import random
    rng = random.Random(seed)

    # Group by layer
    by_layer: dict[str, list[dict]] = defaultdict(list)
    for p in pairs:
        by_layer[p.get("layer", "unknown")].append(p)

    train_set, eval_set = [], []

    print(f"\n  Stratified split (eval â‰¥ {min_eval} or {eval_fraction*100:.0f}% per layer):")
    for layer, layer_pairs in sorted(by_layer.items()):
        rng.shuffle(layer_pairs)
        n_total = len(layer_pairs)
        n_eval  = max(min_eval, int(n_total * eval_fraction))
        n_eval  = min(n_eval, int(n_total * 0.20))  # cap at 20% of layer
        n_eval  = min(n_eval, n_total)               # can't exceed total

        eval_set.extend(layer_pairs[:n_eval])
        train_set.extend(layer_pairs[n_eval:])
        print(f"    {layer:<38} total={n_total:>5,}  eval={n_eval:>4,}  train={n_total - n_eval:>5,}")

    print(f"\n  Total â€” train: {len(train_set):,}  eval: {len(eval_set):,}")
    return train_set, eval_set


def load_pairs(path: Path) -> list[dict]:
    """Load JSONL training pairs from disk."""
    if not path.exists():
        raise FileNotFoundError(f"Dataset not found: {path}\nRun build_dataset.py first.")
    pairs = []
    with open(path, encoding="utf-8") as f:
        for i, line in enumerate(f):
            line = line.strip()
            if not line:
                continue
            try:
                pairs.append(json.loads(line))
            except json.JSONDecodeError as e:
                print(f"  âš ï¸  Line {i+1}: JSON parse error â€” {e}")
    return pairs


def pairs_to_dataset(pairs: list[dict]) -> Dataset:
    """Convert list of dicts â†’ HuggingFace Dataset with 'text' field."""
    formatted = [format_example(p) for p in pairs]
    return Dataset.from_list(formatted)


# â”€â”€ Load model in 4-bit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_model():
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
    )
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
    )
    model.config.use_cache = False  # required for gradient checkpointing

    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
    tokenizer.pad_token    = tokenizer.eos_token
    tokenizer.padding_side = "right"

    return model, tokenizer


# â”€â”€ FIX 1: LoRA config â€” add MLP target modules â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_lora_config() -> LoraConfig:
    """
    target_modules now includes MLP projections (up/down/gate_proj).

    Mistral uses SwiGLU MLP â€” three projection layers:
      gate_proj: computes gating signal
      up_proj:   projects to intermediate dimension
      down_proj: projects back to model dimension

    These layers store factual associations in transformer LLMs.
    For a security-domain model that needs to recall CWEâ†’fix mappings,
    CVEâ†’technique associations, etc., training MLP layers is essential.

    Previous config trained only: q_proj, k_proj, v_proj, o_proj (~21M params)
    This config trains:          above + up_proj, down_proj, gate_proj (~85M params)
    """
    return LoraConfig(
        task_type      = TaskType.CAUSAL_LM,
        r              = 16,
        lora_alpha     = 32,
        lora_dropout   = 0.05,
        bias           = "none",
        target_modules = [
            # Attention projections (original)
            "q_proj", "k_proj", "v_proj", "o_proj",
            # MLP projections (NEW â€” stores factual knowledge)
            "up_proj", "down_proj", "gate_proj",
        ],
    )


# â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    print(f"Loading dataset: {DATASET_PATH}")
    pairs = load_pairs(DATASET_PATH)
    print(f"  Loaded {len(pairs):,} training pairs")

    # FIX 2: Stratified split
    train_pairs, eval_pairs = stratified_split(pairs)
    train_dataset = pairs_to_dataset(train_pairs)
    eval_dataset  = pairs_to_dataset(eval_pairs)

    print(f"\nLoading base model: {BASE_MODEL}")
    model, tokenizer = load_model()

    lora_cfg = get_lora_config()
    model    = get_peft_model(model, lora_cfg)
    model.print_trainable_parameters()

    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    training_args = SFTConfig(
        output_dir                  = str(OUTPUT_DIR),
        num_train_epochs            = 3,
        per_device_train_batch_size = 1,
        gradient_accumulation_steps = 16,
        gradient_checkpointing      = True,
        optim                       = "paged_adamw_8bit",
        learning_rate               = 2e-4,
        lr_scheduler_type           = "cosine",
        warmup_steps                = 100,
        fp16                        = True,
        logging_steps               = 50,
        logging_strategy            = "steps",
        eval_strategy               = "steps",
        eval_steps                  = 200,
        save_steps                  = 200,
        save_total_limit            = 3,
        load_best_model_at_end      = True,
        max_length                  = 2048,
        dataset_text_field          = "text",
        report_to                   = "none",
    )

    trainer = SFTTrainer(
        model            = model,
        args             = training_args,
        train_dataset    = train_dataset,
        eval_dataset     = eval_dataset,
        processing_class = tokenizer,
    )

    print("\nðŸš€ Starting fine-tuning...")
    trainer.train()

    # â”€â”€ Save and Merge â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    final_dir  = OUTPUT_DIR / "final"
    merged_dir = OUTPUT_DIR / "merged"

    trainer.save_model(str(final_dir))
    print(f"\nâœ… LoRA adapter saved â†’ {final_dir}")

    print("\nMerging LoRA weights into base model...")
    merged = model.merge_and_unload()
    merged.save_pretrained(str(merged_dir))
    tokenizer.save_pretrained(str(merged_dir))
    print(f"âœ… Merged model saved â†’ {merged_dir}")

    # â”€â”€ Push to Hub â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\nPushing to HuggingFace Hub: {HF_REPO_NAME}")
    from huggingface_hub import login
    login()

    merged.push_to_hub(HF_REPO_NAME)
    tokenizer.push_to_hub(HF_REPO_NAME)
    print(f"ðŸš€ Model live: https://huggingface.co/{HF_REPO_NAME}")

    from datasets import load_dataset as ld
    full_ds = ld("json", data_files=str(DATASET_PATH), split="train")
    full_ds.push_to_hub(f"{HF_REPO_NAME}-dataset")
    print(f"ðŸš€ Dataset live: https://huggingface.co/datasets/{HF_REPO_NAME}-dataset")


if __name__ == "__main__":
    main()